# First Blog Post: Where do I Start?

I want to start my portfolio with a large project that covers a fair bit of tools and libraries without being too complex or daunting. For this reason I have chosen not to delve into deep learning libraries like TensorFlow or PyTorch just yet. Instead I have a handful of libraries I want to use.

### 1. Pandas, Matplotlib, and Numpy
These three libaries I am very familiar and competent with. I have used Pandas for numerous previous school projects and actually prefer it to excel due to how familiar I am with it. I understand matplotlib very well, but haven't had a chance to use it a lot in independent projects and would like to put together some informative graphs with it. Numpy more or less is a basic data science tool that I also am experienced with and also a good library to demonstrate basic competency with.

### 2. Scikit-Learn
This library is what I am most used to when making basic machine learning models. I have a good understanding of how it can be used for regression, classification, and unsupervised learning tasks. I would also like to try to experiment more with the pipeline functionality that the library has to offer. I also want to report on the metrics side of model performance so I can have a more mathematical understanding of what I am doing in this library.

### 3. XGBoost
As a model not included in scikit-learn, I have experience with this in a previous project I worked on, but wish to examine it further and compare performance with models within scikit-learn.

### 4. PyCaret
This is a library I have zero experience with that I am very curious about. It seems to automate a lot of the modeling process and if effective would be a great tool for me to have under my belt.

## Project Goals and Outline
As of now, the listed libraries are what I would like to focus on. I don't see myself not covering any of these, but it is a very big possibility that there might be another non deep learning library that I want to try that I will include later on in the project.

### Step 1: Data Collection
This step I still am rather blurry on the details of. I am not sure whether I want to create my own datasets using other tools or libraries not listed or just go to kaggle and look for a competition to join and use data from for this project. I plan on needing 2 to 3 data sets. At least one for a regression model and one for a classification model. A third one may be required for demonstrating unsupervised learning model creation.

### Step 2: Data Preparation
Imputation, row dropping, etc. I don't expect this step to be particularly challenging, but I also suspect that my methods may be a little flawed and some oversights may occur. I will experiment with a few data preparation methods for dealing with missing or flawed data and see how they perform on a model.

### Step 3: Model Selection
This is where I expect PyCaret to become a useful tool. I will manually train each model with scikit-learn as well as trying to the PyCaret method the compare the two.

### Step 4: Hyperparameter Tuning
This is where I will use gridsearchCV and randomsearchCV to optimize parameters that I have researched should be tweaked after monitoring the performance of the model with the default parameters set.

### Step 5: Writing a Conclusion
Througout the step I will make sure to be documenting everything and uploading to my github during the whole process, but I plan to upload a full conclusion with what I learned on the blog at the end of the project.

